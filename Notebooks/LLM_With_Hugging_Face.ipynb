{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itinstructor/JupyterNotebooks/blob/main/Notebooks/LLM_With_Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "# Getting Started with Large Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHC-R-SDVvOV"
      },
      "source": [
        "## What is a large language model (LLM)?\n",
        "\n",
        "A large language model (LLM) is a type of artificial intelligence (AI) program that can recognize and generate text, among other tasks. LLMs are trained on huge sets of data — hence the name \"large.\" LLMs are built on machine learning: specifically, a type of neural network called a transformer model.\n",
        "\n",
        "In simpler terms, an LLM is a computer program that has been fed enough examples to be able to recognize and interpret human language or other types of complex data. Many LLMs are trained on data that has been gathered from the Internet — thousands or millions of gigabytes' worth of text. But the quality of the samples impacts how well LLMs will learn natural language, so an LLM's programmers may use a more curated data set.\n",
        "\n",
        "LLMs use a type of machine learning called deep learning in order to understand how characters, words, and sentences function together. Deep learning involves the probabilistic analysis of unstructured data, which eventually enables the deep learning model to recognize distinctions between pieces of content without human intervention.\n",
        "\n",
        "LLMs are then further trained via tuning: they are fine-tuned or prompt-tuned to the particular task that the programmer wants them to do, such as interpreting questions and generating responses, or translating text from one language to another."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hello World\")"
      ],
      "metadata": {
        "id": "c5cRoD6YMljd",
        "outputId": "082f6a25-c40d-4cab-95c5-c6cd665c1ec1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMdOTv-uVvOW"
      },
      "source": [
        "## Transformers in a Nutshell\n",
        "\n",
        "A **transformer** is a neural network architecture based on the concept of **attention**.\n",
        "* They're what make LLMs work - behind ChatGPT et al.\n",
        "* You feed a lot of text data into the neural network, and it learns which words relate to other words\n",
        "\n",
        "\n",
        "<div>\n",
        "    <center>\n",
        "        <table>\n",
        "            <tr>\n",
        "                <td><img src=\"https://github.com/ericmanley/LLM4CSCurriculumWorkshop/blob/main/images/simple_self_attention.png?raw=1\" width=400px></td>\n",
        "                <td><img src=\"https://github.com/ericmanley/LLM4CSCurriculumWorkshop/blob/main/images/attention_vis1.png?raw=1\" width=300px></td>\n",
        "            </tr>\n",
        "        </table>\n",
        "    </center>\n",
        "</div>\n",
        "\n",
        "\n",
        "*image source:* Speech and Language Processing Fig. 10.2, https://web.stanford.edu/~jurafsky/slp3/10.pdf\n",
        "\n",
        "*image source:* from the original paper on transformers - **attention is all you need** https://arxiv.org/pdf/1706.03762.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngHVQiNEVvOX"
      },
      "source": [
        "## Why transformers?\n",
        "\n",
        "Unlike previous neural network architectures, they can be trained *in parallel*.\n",
        "\n",
        "LLMs use big models (take lots of words as input, encodings for lots of word senses, lots of layers for extracti.ng high level features of text, trained on massive amounts of text)\n",
        "\n",
        "<div>\n",
        "    <center>\n",
        "        <img src=\"https://github.com/ericmanley/LLM4CSCurriculumWorkshop/blob/main/images/transformer_encoder_decoder.png?raw=1\" width=300px>\n",
        "    </center>\n",
        "</div>\n",
        "\n",
        "*image source:* Hugging Face NLP Course - **How do transformers work?** https://huggingface.co/learn/nlp-course/chapter1/4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Access Token\n",
        "\n",
        "1. Go to the left toolbar --> Click the Key icon.\n",
        "2. Click --> Add new secret\n",
        "3. Name: HF_TOKEN\n",
        "4. Value: Paste your Hugging Face token\n",
        "5. Actions: Click the Eye icon --> to hide the characters in your token.\n",
        "3. Click Notebook access to turn it on.\n",
        "6. Click the Key icon to close the Secrets panel."
      ],
      "metadata": {
        "id": "aQxLvfEEm8Uy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY7DSbgQVvOX"
      },
      "source": [
        "## Installing the Hugging Face `transformers` library\n",
        "\n",
        "Hugging Face transformers library is already installed in Google Colab. If this next code cell gives an error, don't worry. We are using a CPU to run the LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOav56wCVvOY",
        "outputId": "f2740f11-704c-401c-f02a-183e79f7a36d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install transformers\n",
        "!{sys.executable} -m pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m0TSiYrVvOY"
      },
      "source": [
        "### What is Hugging Face?\n",
        "\n",
        "Hugging Face is a private company\n",
        "* Founded in 2016 by French entrepreneurs Clément Delangue, Julien Chaumond, and Thomas Wolf\n",
        "* Based in New York City\n",
        "\n",
        "Provide popular free, open-source libraries for natural language processing (and other) tasks.\n",
        "\n",
        "Host *hundreds of thousands of models* that you can use in your own programs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7nJvuYQVvOY"
      },
      "source": [
        "## A first tranformers program: the sentiment analysis pipeline\n",
        "\n",
        "**Sentiment analysis** attempts to identify the overall feeling intended by the writer of some text\n",
        "\n",
        "The creators of this model **trained** it on lots of examples of text that were labeled as either *positive* or *negative*\n",
        "\n",
        "A **pipeline** is a series of steps for performing **inference**\n",
        "* tokenize and preprocess the input text (more on this later)\n",
        "* ask the model for a prediction\n",
        "* post-process model's result and turn it into something you can use\n",
        "\n",
        "\n",
        "<div>\n",
        "    <center>\n",
        "        <img src=\"https://github.com/ericmanley/LLM4CSCurriculumWorkshop/blob/main/images/full_nlp_pipeline.svg?raw=1\" width=600px>\n",
        "    </center>\n",
        "</div>\n",
        "\n",
        "image source: https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "wEBOMeAkgsNO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byblL8JJVvOY"
      },
      "source": [
        "* We *are* specifying the kind of task: `sentiment-analysis`\n",
        "* This task analyzes text for the sentiment of the text in the text variable.\n",
        "* We *are not* asking for a specific model, so it picks one of many it has by default\n",
        "* The first time you do this, it will have to download the model - this can take some time depending on your network connection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pwf8spqVvOZ",
        "outputId": "304571d0-245b-4a39-8bd0-8059dbc884db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text to be classified: I love teaching\n",
            "[{'label': 'POSITIVE', 'score': 0.9993025064468384}]\n"
          ]
        }
      ],
      "source": [
        "# Import the pipeline module from the transformers library\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis classifier pipeline using the default pre-trained model\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "# classifier = pipeline(\"text-classification\", model=\"madhurjindal/autonlp-Gibberish-Detector-492513457\")\n",
        "\n",
        "# Enter text to be classified\n",
        "# example: \"I love how easy it is to build sentiment-aware applications with the transformers library!\"\n",
        "text = input(\"Enter text to be classified: \")\n",
        "\n",
        "# Use the created classifier to analyze the sentiment of the given text\n",
        "results = classifier(text)\n",
        "\n",
        "# Print the sentiment analysis results\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YDCStDfVvOa"
      },
      "source": [
        "**Test it out:** Try changing the input to get different labels/scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHGUrvPBVvOa"
      },
      "source": [
        "## Activity: Specifying a model\n",
        "\n",
        "Now try asking for a specific model.\n",
        "\n",
        "Replace one line of code in your earlier example.\n",
        "\n",
        "You can find out more about this model by checking out its model card: https://huggingface.co/SamLowe/roberta-base-go_emotions\n",
        "\n",
        "What are some things you notice about this model that are different than the first one?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OE_cAJ2WVvOa"
      },
      "outputs": [],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\", model=\"SamLowe/roberta-base-go_emotions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO-u82QPVvOa"
      },
      "source": [
        "## Activity: Explore additional models\n",
        "\n",
        "Go to the Hugging Face models page: https://huggingface.co/models\n",
        "* Click `Text Classification`\n",
        "* Find another model that looks interesting to you and try it out\n",
        "* You might be able to find models for spam detection, fake news detection, topic classification, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpzUieZRVvOa"
      },
      "source": [
        "## What about sequence-to-sequence models?\n",
        "\n",
        "The transformers library has models for generating output sequences - long text as input and output\n",
        "* summarization\n",
        "* translation\n",
        "* question answering\n",
        "\n",
        "Example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xypzxZfdVvOd",
        "outputId": "1b8c4103-6bf7-4370-88c9-91754d1eb363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6GTVSwBVvOe"
      },
      "outputs": [],
      "source": [
        "# Article copied from https://www.npr.org/2024/04/02/1242197022/biden-xi-jinping-call-china\n",
        "example_news_article = \"\"\"\n",
        "BEIJING and WASHINGTON, D.C. — President Biden and Chinese leader Xi Jinping held what a senior Biden administration official dubbed a \"check-in\" call on Tuesday, marking the first conversation between the leaders since their face-to-face meeting in California in November.\n",
        "The latest thorn in Taiwan-China tensions: pineapples\n",
        "World\n",
        "The latest thorn in Taiwan-China tensions: pineapples\n",
        "\n",
        "The call touched on everything from Taiwan to the situation on the Korean Peninsula, artificial intelligence and Russia's war in Ukraine.\n",
        "\n",
        "According to the Chinese readout, Xi told Biden strategic awareness \"must always be the first 'button' to be fastened\" in bilateral ties. The Chinese leader also elaborated his position on issues concerning Hong Kong, human rights and the South China Sea, the readout says.\n",
        "Taiwan's election was a vote for continuity, but adds uncertainty in ties with China\n",
        "World\n",
        "Taiwan's election was a vote for continuity, but adds uncertainty in ties with China\n",
        "\n",
        "The Chinese leader warned again that the \"Taiwan issue\" is an \"insurmountable red line\" in bilateral ties. Xi also urged Biden to \"translate\" his commitment of not supporting \"Taiwan independence\" into concrete actions, according to the readout.\n",
        "\n",
        "Biden, in the call, emphasized the importance of maintaining peace and stability across the Taiwan Strait and the rule of law and freedom of navigation in the South China Sea, according to a White House readout.\n",
        "\n",
        "The two leaders also discussed the global geopolitical situation. Biden, according to the White House, raised concerns over China's support for Russia's defense industrial base and its impact on European and transatlantic security. He also emphasized Washington's \"enduring commitment\" to the complete denuclearization of the Korean Peninsula.\n",
        "\n",
        "Tuesday's call was the first time Biden and Xi have talked since they met in northern California in November. There, they agreed on a range of steps to try to prevent increasingly fraught U.S.-China ties from slipping into conflict, including more frequent contact at the leader level, between militaries and beyond.\n",
        "\n",
        "Ahead of the call, a senior administration official told reporters the conversation would not represent a change in U.S. policy toward China, and competition remains a key feature.\n",
        "\n",
        "\"Intense competition requires intense diplomacy to manage tensions, address misperceptions and prevent unintended conflict. And this call is one way to do that,\" said the official, who spoke on condition of anonymity as he was not permitted to speak on the record.\n",
        "\n",
        "Biden raised perennial U.S. concerns about China's \"unfair trade policies and non-market economic practices,\" according to the White House readout — an issue that will be front and center when Treasury Secretary Janet Yellen visits China later this week.\n",
        "\n",
        "The president also reiterated to his Chinese counterpart that Washington will continue to \"take necessary actions to prevent advanced U.S. technologies from being used to undermine our national security, without unduly limiting trade and investment,\" the White House readout said.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEBfwj4zVvOf",
        "outputId": "b114adec-d379-4f11-ab79-e7f3018a2104",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': ' President Biden and Chinese leader Xi Jinping held what a senior Biden administration official dubbed a \"check-in\" call on Tuesday . The call touched on everything from Taiwan to the situation on the Korean Peninsula, artificial intelligence and Russia\\'s war in Ukraine . Tuesday\\'s call was the first time Biden and Xi have talked since they met in northern California in November .'}]\n"
          ]
        }
      ],
      "source": [
        "summary = summarizer(example_news_article)\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocdfd3BpVvOf"
      },
      "source": [
        "## What about chat bots?\n",
        "\n",
        "Chat bots need models that have been trained on conversational text.\n",
        "\n",
        "To get the next response in a conversational thread, you need to pass in the entire conversation up to that point.\n",
        "\n",
        "Models often use special tokens like `<s>` and `</s>` to indicate where a sequence begins and ends, but it is different for different models: https://huggingface.co/docs/transformers/en/model_doc/blenderbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RHSx1Dv8VvOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d8fd9ec-3884-4d8b-860c-48c0ef661fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "text_gen = pipeline(\"text2text-generation\", model=\"facebook/blenderbot-400M-distill\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-_Zuw9pKVvOf",
        "outputId": "624f9c62-509c-4625-ad9c-7816d10c9f50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask me a question: What is computer science\n",
            "[{'generated_text': ' Computer science is a branch of mathematics that deals with computing.'}]\n"
          ]
        }
      ],
      "source": [
        "conversation = \"<s>What is computer science?</s>\"\n",
        "result = text_gen(conversation)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtcjpr5-VvOg",
        "outputId": "bb878233-b30e-48fa-8f52-1a9bab939398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': ' Yes, it is the study of algorithms and the theory of computation.'}]\n"
          ]
        }
      ],
      "source": [
        "conversation += \"<s>\"+result[0][\"generated_text\"]+\"</s>\"\n",
        "conversation += \"<s>Is it only related to math?</s>\"\n",
        "result = text_gen(conversation)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSb3ykP8VvOg",
        "outputId": "d593f7a2-3d86-42ec-f4ee-03dbe16501f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>What is computer science?</s><s> Computer science is a branch of mathematics that deals with computing.</s><s>Is it only related to math?</s><s> Yes, it is the study of algorithms and the theory of computation.</s>\n"
          ]
        }
      ],
      "source": [
        "conversation += \"<s>\"+result[0][\"generated_text\"]+\"</s>\"\n",
        "print(conversation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Challenge:** Take the above code and create a chatbot"
      ],
      "metadata": {
        "id": "i3F0dIwag98L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "text_gen = pipeline(\"text2text-generation\", model=\"facebook/blenderbot-400M-distill\")\n",
        "conversation = input(\"Ask me a question: \")\n",
        "# Add delimiters\n",
        "conversation = \"<s>\" + conversation + \"</s>\"\n",
        "result = text_gen(conversation)\n",
        "print(result)\n",
        "# Continue from here to create a conversation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4zfq7emhF5-",
        "outputId": "4bccccec-d1a4-4107-c93c-29e4c629668f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask me a question: What is candy\n",
            "[{'generated_text': ' Candy is a sweet treat that is usually sweetened with sugar or sugar substitutes.'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_EM48NvVvOj"
      },
      "source": [
        "## Resources\n",
        "\n",
        "Free NLP Textbook: Speech and Language Processing by Dan Jurafsky and James H. Martin\n",
        "* https://web.stanford.edu/~jurafsky/slp3/\n",
        "* great for theoretical and intuitive understanding of concepts\n",
        "\n",
        "Hugging Face NLP Course: https://huggingface.co/learn/nlp-course/\n",
        "* great for engineering/implementation\n",
        "\n",
        "Course Materials: https://github.com/ericmanley/F23-CS195NLP\n",
        "* Natural Language Processing course for undergrads that includes lots of implementation\n",
        "* Includes Jupyter Notebooks like this one\n",
        "\n",
        "Fine-Tuning Models for new data\n",
        "* Hugging Face fine-tuning chapter: https://huggingface.co/learn/nlp-course/chapter3/1\n",
        "* From my NLP course: https://github.com/ericmanley/F23-CS195NLP/blob/main/F7_1_TransferLearning.ipynb\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}